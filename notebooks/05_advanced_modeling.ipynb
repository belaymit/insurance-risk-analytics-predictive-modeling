{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Modeling and Hyperparameter Optimization\n",
        "\n",
        "## Overview\n",
        "This notebook extends the basic modeling from notebook 04 with:\n",
        "- Hyperparameter tuning using GridSearchCV\n",
        "- Cross-validation for robust model evaluation\n",
        "- Advanced ensemble methods\n",
        "- Model stacking\n",
        "- Production-ready model pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Advanced modeling libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# import xgboost as xgb\n",
        "# from scipy.stats import randint, uniform\n",
        "\n",
        "# Set random state for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Advanced modeling libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing data...\n",
            "Dataset loaded: (1000098, 61)\n",
            "Features for modeling: 50 (16 numerical + 34 categorical)\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare data (using the same preprocessing from notebook 04)\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Load and prepare data for advanced modeling\"\"\"\n",
        "    \n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('../MachineLearningRating_v3.txt', delimiter='|', low_memory=False)\n",
        "    \n",
        "    # Feature engineering (same as notebook 04)\n",
        "    df['HasClaim'] = (df['TotalClaims'] > 0).astype(int)\n",
        "    df['ClaimRatio'] = df['TotalClaims'] / (df['TotalPremium'] + 1e-6)\n",
        "    df['PremiumPerInsured'] = df['TotalPremium'] / (df['SumInsured'] + 1e-6)\n",
        "    df['VehicleAge'] = (2024 - df['RegistrationYear']).clip(0, 50)\n",
        "    df['Gender_MaritalStatus'] = df['Gender'].astype(str) + '_' + df['MaritalStatus'].astype(str)\n",
        "    df['Province_VehicleType'] = df['Province'].astype(str) + '_' + df['VehicleType'].astype(str)\n",
        "    \n",
        "    # Risk scores\n",
        "    province_risk = df.groupby('Province')['HasClaim'].mean()\n",
        "    df['ProvinceRiskScore'] = df['Province'].map(province_risk)\n",
        "    \n",
        "    make_risk = df.groupby('make')['HasClaim'].mean()\n",
        "    df['MakeRiskScore'] = df['make'].map(make_risk)\n",
        "    \n",
        "    df['IsNewDriver'] = ((df['Gender'] == 'Male') & (df['VehicleAge'] < 5)).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "print(\"Loading and preparing data...\")\n",
        "df = load_and_prepare_data()\n",
        "print(f\"Dataset loaded: {df.shape}\")\n",
        "\n",
        "# Prepare feature sets\n",
        "exclude_cols = ['PolicyID', 'TotalClaims', 'TotalPremium', 'HasClaim', 'ClaimRatio', \n",
        "               'UnderwrittenCoverID', 'TransactionMonth']\n",
        "\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numerical_features = [col for col in numerical_cols if col not in exclude_cols]\n",
        "\n",
        "categorical_features = []\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    if col not in exclude_cols and df[col].nunique() < 50:\n",
        "        categorical_features.append(col)\n",
        "\n",
        "all_features = numerical_features + categorical_features\n",
        "print(f\"Features for modeling: {len(all_features)} ({len(numerical_features)} numerical + {len(categorical_features)} categorical)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameter grids and preprocessing pipeline created.\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning configurations\n",
        "def get_param_grids():\n",
        "    \"\"\"Define parameter grids for hyperparameter tuning\"\"\"\n",
        "    \n",
        "    param_grids = {\n",
        "        'RandomForestRegressor': {\n",
        "            'regressor__n_estimators': [100, 200, 300],\n",
        "            'regressor__max_depth': [10, 15, 20, None],\n",
        "            'regressor__min_samples_split': [2, 5, 10],\n",
        "            'regressor__min_samples_leaf': [1, 2, 4],\n",
        "            'regressor__max_features': ['sqrt', 'log2', None]\n",
        "        },\n",
        "        'XGBRegressor': {\n",
        "            'regressor__n_estimators': [100, 200, 300],\n",
        "            'regressor__max_depth': [6, 8, 10],\n",
        "            'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'regressor__subsample': [0.8, 0.9, 1.0],\n",
        "            'regressor__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        },\n",
        "        'RandomForestClassifier': {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [10, 15, 20, None],\n",
        "            'classifier__min_samples_split': [2, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None]\n",
        "        },\n",
        "        'XGBClassifier': {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [6, 8, 10],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
        "            'classifier__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return param_grids\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "def create_preprocessor(numerical_features, categorical_features):\n",
        "    \"\"\"Create preprocessing pipeline\"\"\"\n",
        "    \n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "    \n",
        "    return preprocessor\n",
        "\n",
        "preprocessor = create_preprocessor(numerical_features, categorical_features)\n",
        "param_grids = get_param_grids()\n",
        "\n",
        "print(\"Hyperparameter grids and preprocessing pipeline created.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
